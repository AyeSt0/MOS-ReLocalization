# scripts/ai_apply_namemap_unify.py
import os
import json
import re
import asyncio
import time
from pathlib import Path
from dotenv import load_dotenv
from contextlib import asynccontextmanager
from openai import AsyncOpenAI

# ========== çŽ¯å¢ƒåŠ è½½ ==========
load_dotenv()

DATA_PATH = Path("output/language_dict_mcname_fixed.json")
LANG_MAP_PATH = Path("data/language_map.json")
NAME_MAP_PATH = Path("data/name_map.json")
OUTPUT_PATH = Path("output/language_dict_namemap_applied.json")
REPORT_PATH = Path("output/namemap_apply_report.txt")

# æ¨¡åž‹ä¸Žé™é€Ÿé…ç½®
DEFAULT_PROVIDER = os.getenv("MODEL_PROVIDER", "deepseek").strip().lower()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_MODEL = os.getenv("MODEL", "gpt-4o-mini")
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "")
DEEPSEEK_BASE_URL = os.getenv("DEEPSEEK_BASE_URL", "https://api.siliconflow.cn")
DEEPSEEK_MODEL = os.getenv("DEEPSEEK_MODEL", "deepseek-ai/DeepSeek-V3.2-Exp")

RPM = int(os.getenv("RPM", "700"))
TPM = int(os.getenv("TPM", "80000"))
ASYNC_CONCURRENCY = int(os.getenv("ASYNC_CONCURRENCY", "30"))
BATCH_FLUSH = int(os.getenv("BATCH_FLUSH", "200"))
PRINT_EVERY = int(os.getenv("PRINT_EVERY", "50"))

# ========== å·¥å…·å‡½æ•° ==========
def load_json(path, default=None):
    if default is None:
        default = {}
    if not path.exists():
        return default
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)

def save_json(path, data):
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp = path.with_suffix(path.suffix + ".tmp")
    with tmp.open("w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    tmp.replace(path)

# ========== é™é€Ÿå™¨ ==========
class RateLimiter:
    def __init__(self, rpm, tpm):
        self.rpm = rpm
        self.tpm = tpm
        self.reset_time = time.monotonic()
        self.req = 0
        self.tok = 0
        self.lock = asyncio.Lock()

    def _maybe_reset(self):
        now = time.monotonic()
        if now - self.reset_time >= 60:
            self.reset_time = now
            self.req = 0
            self.tok = 0

    def _estimate_tokens(self, text):
        return max(8, int(len(text) / 3.5) + 50)

    async def acquire(self, text):
        est = self._estimate_tokens(text)
        while True:
            async with self.lock:
                self._maybe_reset()
                if self.req + 1 <= self.rpm and self.tok + est <= self.tpm:
                    self.req += 1
                    self.tok += est
                    return
                wait = max(0.1, 60 - (time.monotonic() - self.reset_time))
            await asyncio.sleep(wait)

# ========== å®¢æˆ·ç«¯ ==========
@asynccontextmanager
async def build_async_client(provider: str):
    if provider == "deepseek":
        client = AsyncOpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL)
        model = DEEPSEEK_MODEL
        yield client, model
        await client.close()
    else:
        client = AsyncOpenAI(api_key=OPENAI_API_KEY)
        model = OPENAI_MODEL
        yield client, model
        await client.close()

def choose_provider():
    print("\nè¯·é€‰æ‹©å¼•æ“Žï¼ˆ1=ChatGPTï¼Œ2=DeepSeekï¼‰ï¼š", end="")
    choice = input().strip()
    if choice == "1":
        print("ðŸ§  ä½¿ç”¨ CHATGPT æ¨¡åž‹å¼•æ“Ž")
        return "chatgpt"
    print("ðŸ§  ä½¿ç”¨ DEEPSEEK æ¨¡åž‹å¼•æ“Ž")
    return "deepseek"

# ========== Prompt æž„å»º ==========
def build_prompt(ru_text, en_text, zh_text, name_map):
    map_text = "\n".join([f"{k}: {v}" for k, v in name_map.items()])
    return f"""
ä½ æ˜¯æ¸¸æˆæœ¬åœ°åŒ–ç¼–è¾‘ï¼Œä»»åŠ¡æ˜¯ç»Ÿä¸€ä¸“æœ‰åè¯è¯‘åã€‚

ä»¥ä¸‹æ˜¯ä¸‰è¯­å¯¹ç…§çš„æ–‡æœ¬ï¼š
---
ä¿„æ–‡: {ru_text}
è‹±æ–‡: {en_text}
ä¸­æ–‡: {zh_text}
---

å·²çŸ¥ç»Ÿä¸€ä¸“åè¡¨ï¼ˆä¸å¯æ›´æ”¹ï¼‰ï¼š
{map_text}

è¯·æ£€æŸ¥ä¸­æ–‡å¥å­ä¸­çš„ä¸“åæ˜¯å¦å­˜åœ¨è¯‘æ³•ä¸ç»Ÿä¸€ã€é—æ¼æˆ–æ··ä¹±çš„æƒ…å†µã€‚
å¦‚æžœéœ€è¦ä¿®æ­£ï¼Œè¯·ç”¨ç»Ÿä¸€è¯‘æ³•æ›¿æ¢é”™è¯¯çš„éƒ¨åˆ†ï¼Œä½¿æ•´ä½“æµç•…è‡ªç„¶ã€‚
åªè¿”å›žä¿®æ­£åŽçš„ä¸­æ–‡è¯‘æ–‡ï¼ˆä¸å¾—æ·»åŠ è§£é‡Šã€æ‹¬å·ã€è¯´æ˜Žæˆ–å¼•å·ï¼‰ã€‚
""".strip()

# ========== æ¸…æ´—å‡½æ•° ==========
def clean_output(txt: str):
    if not txt:
        return ""
    txt = txt.strip().strip("`").strip()
    lines = [l.strip() for l in txt.splitlines() if l.strip()]
    return lines[0] if lines else ""

# ========== ä¸»é€»è¾‘ ==========
async def main_async():
    provider = choose_provider()
    data = load_json(DATA_PATH)
    lang_map = load_json(LANG_MAP_PATH)
    name_map = load_json(NAME_MAP_PATH)

    total = len(data)
    print(f"âœ… æ•°æ®è½½å…¥æˆåŠŸï¼šå…± {total} æ¡ã€‚")

    # å®šä½åˆ—
    ru_col = next((int(k) for k, v in lang_map.items() if "Russian" in v), 1)
    en_col = next((int(k) for k, v in lang_map.items() if "English" in v), 2)
    zh_col = next((int(k) for k, v in lang_map.items() if "Chinese" in v), 5)
    print(f"ä¿„æ–‡åˆ—={ru_col}ï¼Œè‹±æ–‡åˆ—={en_col}ï¼Œä¸­æ–‡åˆ—={zh_col}")
    print(f"ðŸ“˜ name_map ä¸­æœ‰ {len(name_map)} æ¡ä¸“åæ˜ å°„ã€‚")

    # æ£€æµ‹å€™é€‰
    candidates = []
    for k, row in data.items():
        if len(row) <= max(ru_col, en_col, zh_col):
            continue
        ru, en, zh = row[ru_col], row[en_col], row[zh_col]
        if not zh.strip():
            continue
        for n in name_map.keys():
            if n in ru or n in en:
                candidates.append((k, ru, en, zh))
                break

    print(f"ðŸ“¦ å¾…ä¿®æ­£å¥å­æ•°ï¼š{len(candidates)}ï¼ˆä»…æ£€æµ‹è‹±æ–‡/ä¿„æ–‡å«ä¸“åè¡Œï¼‰")

    limiter = RateLimiter(RPM, TPM)
    sem = asyncio.Semaphore(ASYNC_CONCURRENCY)
    report_lines = []
    processed = 0

    async with build_async_client(provider) as (client, model):
        async def worker(i, key, ru, en, zh):
            nonlocal processed
            try:
                await limiter.acquire(zh)
                sys_msg = "You are a professional localization QA editor. Output translation only."
                user_msg = build_prompt(ru, en, zh, name_map)
                resp = await client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": sys_msg},
                        {"role": "user", "content": user_msg},
                    ],
                    temperature=0.4,
                    timeout=30,
                )
                out = clean_output(resp.choices[0].message.content)
                if out and out != zh:
                    data[key][zh_col] = out
                    report_lines.append(f"ã€{key}ã€‘ä¿®æ­£:\nåŽŸ:{zh}\næ–°:{out}\n")
            except Exception as e:
                if "429" in str(e):
                    await asyncio.sleep(5)
                else:
                    report_lines.append(f"âš ï¸ ç¬¬{i}æ¡å‡ºé”™ï¼š{type(e).__name__} â†’ {e}")
            processed += 1
            if processed % PRINT_EVERY == 0:
                print(f"â³ [{processed}/{len(candidates)}] {en[:40]} â†’ {data[key][zh_col][:40]}")
            if processed % BATCH_FLUSH == 0:
                save_json(OUTPUT_PATH, data)
                print("ðŸ’¾ è‡ªåŠ¨ä¿å­˜è¿›åº¦...")

        await asyncio.gather(*(worker(i, k, r, e, c) for i, (k, r, e, c) in enumerate(candidates, 1)))

    save_json(OUTPUT_PATH, data)
    with open(REPORT_PATH, "w", encoding="utf-8") as f:
        f.write("\n".join(report_lines))
    print(f"\nâœ… ä¿®æ­£å®Œæˆï¼Œå…± {processed} æ¡ã€‚")
    print(f"ðŸ“˜ æŠ¥å‘Š: {REPORT_PATH}")
    print(f"ðŸ“ è¾“å‡ºæ–‡ä»¶: {OUTPUT_PATH}")

def main():
    asyncio.run(main_async())

if __name__ == "__main__":
    main()
