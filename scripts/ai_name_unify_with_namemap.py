# scripts/ai_name_unify_with_namemap.py
import os
import re
import json
import time
import math
import signal
import asyncio
from pathlib import Path
from typing import Dict, Tuple, List, Set
from contextlib import asynccontextmanager
from dotenv import load_dotenv

# ================== ç¯å¢ƒä¸è·¯å¾„ ==================
load_dotenv()

# æ•°æ®è¾“å…¥ï¼ˆåªæ”¹ä¸­æ–‡åˆ—ï¼‰
DATA_PATH        = Path(os.getenv("UNIFY_DATA_PATH", "output/language_dict_mcname_fixed.json"))
LANG_MAP_PATH    = Path(os.getenv("LANG_MAP_PATH", "data/language_map.json"))
NAME_MAP_PATH    = Path(os.getenv("NAME_MAP_PATH", "data/name_map.json"))
OUTPUT_PATH      = Path(os.getenv("UNIFY_OUTPUT_PATH", "output/language_dict_name_unified.json"))
REPORT_PATH      = Path(os.getenv("UNIFY_REPORT_PATH", "output/name_unify_report.txt"))

# ç¼“å­˜
CACHE_DIR        = Path(os.getenv("CACHE_DIR", "cache"))
CACHE_PATH       = CACHE_DIR / "name_unify_cache.json"

# å¼•æ“é€‰æ‹©ï¼ˆé»˜è®¤è¯»å– .envï¼‰
DEFAULT_PROVIDER = os.getenv("MODEL_PROVIDER", "").strip().lower()  # chatgpt / deepseek
OPENAI_API_KEY   = os.getenv("OPENAI_API_KEY", "").strip()
OPENAI_MODEL     = os.getenv("MODEL", "gpt-4o-mini").strip()
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "").strip()
DEEPSEEK_BASEURL = os.getenv("DEEPSEEK_BASE_URL", "https://api.siliconflow.cn").strip()
DEEPSEEK_MODEL   = os.getenv("DEEPSEEK_MODEL", "deepseek-ai/DeepSeek-R1").strip()

# å¹¶å‘ä¸é™é€Ÿï¼ˆDeepSeek: RPM=1000, TPM=100000ï¼‰
ASYNC_CONCURRENCY = int(os.getenv("UNIFY_ASYNC_CONCURRENCY", "120"))
REQUEST_TIMEOUT   = int(os.getenv("UNIFY_REQUEST_TIMEOUT", "30"))
RPM               = int(os.getenv("UNIFY_RPM", "1000"))
TPM               = int(os.getenv("UNIFY_TPM", "100000"))

# æ‰“å°/è½ç›˜
PRINT_EVERY       = int(os.getenv("UNIFY_PRINT_EVERY", "50"))
FLUSH_EVERY       = int(os.getenv("UNIFY_FLUSH_EVERY", "500"))

# å€™é€‰ç­›é€‰ï¼ˆä»… 1~2 è¯çš„çŸ­ä¸“åï¼‰
MAX_TOKENS_IN_PHRASE = 2

# è‹±æ–‡/ä¿„æ–‡è¯æ­£åˆ™ï¼ˆä»…å­—æ¯/è¿å­—ç¬¦/å¥ç‚¹ï¼‰
WORD_RE = re.compile(r"[A-Za-zĞ-Ğ¯Ğ°-ÑĞÑ‘][A-Za-zĞ-Ğ¯Ğ°-ÑĞÑ‘.\-']+")

# å¿½ç•¥è¯ï¼ˆç§°è°“/å† è¯ç­‰ï¼›ä¸å½“ä½œä¸“åï¼‰
STOPWORDS_EN = {
    "the","a","an","mr","mrs","ms","miss","dr","prof","professor","coach",
    "sir","madam","lady","lord","captain","principal","dean","ok","tv","id","usa","eu"
}
STOPWORDS_RU = {
    "Ğ¼Ğ¸ÑÑ‚ĞµÑ€","Ğ¼Ğ¸ÑÑĞ¸Ñ","Ğ¼Ğ¸ÑÑ","Ğ¿Ñ€Ğ¾Ñ„","Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¾Ñ€","Ğ³Ğ¾ÑĞ¿Ğ¾Ğ´Ğ¸Ğ½","Ğ³Ğ¾ÑĞ¿Ğ¾Ğ¶Ğ°","ĞºĞ°Ğ¿Ğ¸Ñ‚Ğ°Ğ½","Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€"
}

# åªå¯¹ä¸­æ–‡åˆ—è¿›è¡Œä¿®æ”¹
TARGET_LANG_KEYWORD = "Chinese"  # ç”¨äº language_map.json æŸ¥ä¸­æ–‡åˆ—

stop_requested = False

# ================== åŸºç¡€å·¥å…· ==================
def load_json(path: Path, default=None):
    if default is None:
        default = {}
    if not path.exists():
        return default
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)

def save_json(path: Path, data):
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp = path.with_suffix(path.suffix + ".tmp")
    with tmp.open("w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    tmp.replace(path)

def ensure_row_len(row: list, idx: int):
    if len(row) <= idx:
        row.extend([""] * (idx - len(row) + 1))

def pick_col_by_lang(lang_map: Dict[str, str], label_contains: str) -> int:
    for k, v in lang_map.items():
        if v and label_contains.lower() in v.lower():
            return int(k)
    return -1

def tokenize(text: str) -> List[str]:
    return WORD_RE.findall(text or "")

def normalize_token(tok: str) -> str:
    return re.sub(r"[^\w\-\.']", "", tok).strip().lower()

def is_stopword(tok_norm: str, is_ru: bool) -> bool:
    return tok_norm in (STOPWORDS_RU if is_ru else STOPWORDS_EN)

def chunked(iterable, size):
    buf = []
    for x in iterable:
        buf.append(x)
        if len(buf) >= size:
            yield buf
            buf = []
    if buf:
        yield buf

def eta_str(done:int, total:int, start_ts:float) -> str:
    if done == 0:
        return "--:--"
    elapsed = time.time() - start_ts
    rate = done / max(1e-9, elapsed)
    remain = max(0, total - done) / max(1e-9, rate)
    m, s = divmod(int(remain), 60)
    return f"{m}m{s:02d}s"

# ================== ä¿¡å·å®‰å…¨é€€å‡º ==================
def handle_signal(signum, frame):
    global stop_requested
    stop_requested = True
    print("\nâš ï¸ æ•è·ä¸­æ–­ä¿¡å·ï¼Œå‡†å¤‡å®‰å…¨è½ç›˜é€€å‡ºâ€¦")

signal.signal(signal.SIGINT, handle_signal)
signal.signal(signal.SIGTERM, handle_signal)

# ================== Rate Limiterï¼ˆè‡ªé€‚åº”ï¼‰ ==================
class RateLimiter:
    def __init__(self, rpm:int, tpm:int):
        self.rpm = rpm
        self.tpm = tpm
        self._lock = asyncio.Lock()
        self._win_start = time.monotonic()
        self._req_used = 0
        self._tok_used = 0
        # è‡ªé€‚åº”å‚æ•°
        self._cooldown = 0.0   # çŸ­æ—¶å†·å´é™„åŠ 
        self._scale = 1.0      # åŠ¨æ€è°ƒé€Ÿç³»æ•°ï¼ˆè¶Šå°è¶Šæ…¢ï¼‰

    def _maybe_reset(self):
        now = time.monotonic()
        if now - self._win_start >= 60.0:
            self._win_start = now
            self._req_used = 0
            self._tok_used = 0
            # é€æ­¥æ¢å¤é€Ÿåº¦
            self._scale = min(1.0, self._scale * 1.05)
            self._cooldown = max(0.0, self._cooldown * 0.7)

    @staticmethod
    def estimate_tokens(prompt_len:int) -> int:
        # ç²—ä¼° tokenï¼šç»Ÿä¸€æŒ‰ 3.5 å­—ç¬¦/Tokenï¼Œæœ€ä½ 16
        return max(16, int(prompt_len / 3.5))

    async def acquire(self, prompt_len:int):
        need_tokens = self.estimate_tokens(prompt_len)
        while True:
            async with self._lock:
                self._maybe_reset()
                rpm_cap = max(1, int(self.rpm * self._scale))
                tpm_cap = max(512, int(self.tpm * self._scale))
                can_req = (self._req_used + 1) <= rpm_cap
                can_tok = (self._tok_used + need_tokens) <= tpm_cap
                if can_req and can_tok:
                    self._req_used += 1
                    self._tok_used += need_tokens
                    cd = self._cooldown
                    self._cooldown = max(0.0, self._cooldown * 0.8)
                    # å†·å´å»¶æ—¶ï¼ˆå‘½ä¸­429åä¼šå‡é«˜ï¼‰
                    if cd > 0:
                        await asyncio.sleep(min(2.0, cd))
                    return
                wait = max(0.02, 60.0 - (time.monotonic() - self._win_start))
            await asyncio.sleep(min(1.0, wait))

    async def penalize(self):
        # å‘½ä¸­é™é€Ÿ -> ç«‹åˆ»é™é€Ÿ + å¢åŠ çŸ­å†·å´
        async with self._lock:
            self._scale = max(0.2, self._scale * 0.85)
            self._cooldown = min(2.0, self._cooldown + 0.2)

# ================== ç¼“å­˜ ==================
class Cache:
    def __init__(self, path: Path):
        self.path = path
        self.path.parent.mkdir(parents=True, exist_ok=True)
        self.data = load_json(path, {})
        self._lock = asyncio.Lock()

    def _key(self, provider:str, model:str, ru:str, en:str, cn:str) -> str:
        # ç®€å•é”®ï¼ˆè¶³å¤Ÿå”¯ä¸€ï¼‰
        raw = f"{provider}|{model}|{ru}|{en}|{cn}"
        return str(abs(hash(raw)))

    async def get(self, provider:str, model:str, ru:str, en:str, cn:str):
        k = self._key(provider, model, ru, en, cn)
        async with self._lock:
            return self.data.get(k, "")

    async def set(self, provider:str, model:str, ru:str, en:str, cn:str, value:str):
        k = self._key(provider, model, ru, en, cn)
        async with self._lock:
            if k not in self.data:
                self.data[k] = value

    async def flush(self):
        async with self._lock:
            save_json(self.path, self.data)

# ================== OpenAI Async å®¢æˆ·ç«¯ ==================
@asynccontextmanager
async def build_async_client(provider:str):
    from openai import AsyncOpenAI
    if provider == "deepseek":
        if not DEEPSEEK_API_KEY:
            raise RuntimeError("ç¼ºå°‘ DEEPSEEK_API_KEY")
        client = AsyncOpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASEURL)
        model = DEEPSEEK_MODEL
        try:
            yield client, model
        finally:
            await client.close()
    else:
        if not OPENAI_API_KEY:
            raise RuntimeError("ç¼ºå°‘ OPENAI_API_KEY")
        client = AsyncOpenAI(api_key=OPENAI_API_KEY)
        model = OPENAI_MODEL
        try:
            yield client, model
        finally:
            await client.close()

def choose_provider() -> str:
    print("è¯·é€‰æ‹©å¼•æ“ï¼ˆ1=ChatGPTï¼Œ2=DeepSeekï¼‰ï¼š", end="")
    choice = input().strip()
    if choice == "2" or (not choice and DEFAULT_PROVIDER == "deepseek"):
        print("ğŸ§  ä½¿ç”¨ DEEPSEEK æ¨¡å‹å¼•æ“")
        return "deepseek"
    print("ğŸ§  ä½¿ç”¨ CHATGPT æ¨¡å‹å¼•æ“")
    return "chatgpt"

# ================== AI åˆ¤å®š & æ›¿æ¢ ==================
def build_ai_prompt(ru: str, en: str, cn: str) -> str:
    return f"""ä½ æ˜¯ä¸‰è¯­æœ¬åœ°åŒ–ä¸€è‡´æ€§å®¡æ ¡ä¸“å®¶ã€‚ä¸‹é¢æ˜¯æˆäººå‘è§†è§‰å°è¯´çš„ä¸€æ¡ä¸‰è¯­æ–‡æœ¬ï¼ˆä¿„/è‹±/ä¸­ï¼‰ï¼š

ä¿„æ–‡ï¼š{ru}
è‹±æ–‡ï¼š{en}
ä¸­æ–‡ï¼š{cn}

ä»»åŠ¡ï¼šè¯†åˆ«å…¶ä¸­åº”å½“ç»Ÿä¸€çš„â€œä¸“æœ‰åè¯ï¼ˆäººåã€åœ°åã€ç»„ç»‡åã€å”¯ä¸€ç§°å‘¼ï¼‰â€ï¼Œå¹¶ç»™å‡ºå…¨å±€ç»Ÿä¸€çš„ä¸­æ–‡è¯‘æ³•ã€‚

è¦æ±‚ï¼š
- ä»…é’ˆå¯¹ 1~2 ä¸ªè¯çš„çŸ­è¯­ï¼ˆä¾‹å¦‚ï¼šRoseã€Miss Youngã€Sunvilleã€Professor Richardsonï¼‰
- è‹¥æŸä¸“ååœ¨ä¸­æ–‡ä¸­å‡ºç°å¤šç§è¯‘æ³•æˆ–æœ‰æ®‹ç•™è‹±æ–‡/ä¿„æ–‡ï¼Œè¯·é€‰æ‹©**æœ€è‡ªç„¶ã€æœ¬åœ°åŒ–ã€ç»Ÿä¸€**çš„ä¸­æ–‡ç‰ˆæœ¬
- ä»…è¾“å‡º JSONï¼ˆé”®ä¸ºåŸå§‹è‹±æ–‡æˆ–ä¿„æ–‡çŸ­è¯ï¼Œå€¼ä¸ºæœ€ç»ˆä¸­æ–‡ç»Ÿä¸€è¯‘åï¼‰
- å¦‚æ— å¯ç»Ÿä¸€é¡¹ï¼Œè¾“å‡º {{}}
- ä¸¥ç¦è¾“å‡ºä»»ä½•è§£é‡Šæˆ–é™„åŠ æ–‡æœ¬
"""

def clean_json_only(s: str) -> Dict[str, str]:
    if not s:
        return {}
    s = s.strip().strip("`").strip()
    # æˆªå–ç¬¬ä¸€ä¸ª {...}
    m = re.search(r"\{.*\}", s, flags=re.S)
    if not m:
        return {}
    try:
        obj = json.loads(m.group(0))
        if isinstance(obj, dict):
            # ä»…ä¿ç•™ str->str
            clean = {}
            for k, v in obj.items():
                if isinstance(k, str) and isinstance(v, str):
                    k1 = k.strip()
                    v1 = v.strip()
                    if k1 and v1:
                        clean[k1] = v1
            return clean
    except Exception:
        return {}
    return {}

def replace_pairs_in_text(text: str, pairs: Dict[str, str]) -> str:
    # é•¿è¯ä¼˜å…ˆï¼Œå¿½ç•¥å¤§å°å†™ï¼›ä¸è·¨æ ‡ç­¾
    if not pairs:
        return text
    out = text
    for src, tgt in sorted(pairs.items(), key=lambda kv: -len(kv[0])):
        if not src or not tgt:
            continue
        # åªåšç›´æ¥å­ä¸²æ›¿æ¢ï¼ˆè¿™é‡Œä¸­æ–‡åˆ—ï¼Œé€šå¸¸æ— å˜é‡å†²çªï¼‰
        out = re.sub(re.escape(src), tgt, out, flags=re.IGNORECASE)
    return out

# å€™é€‰çŸ­è¯­æŠ½å–ï¼ˆ1~2è¯ï¼‰ï¼Œä»ä¿„/è‹±å„æŠ“ï¼Œå†ä¸ä¸­æ–‡è¿›è¡Œå¯¹é½åˆ¤æ–­
def extract_short_candidates(ru: str, en: str) -> Set[str]:
    cands: Set[str] = set()
    # è‹±æ–‡
    toks_en = tokenize(en)
    buf = [t for t in toks_en if t]
    # ä¿„æ–‡
    toks_ru = tokenize(ru)
    buf_ru = [t for t in toks_ru if t]

    def add_phrases(tokens: List[str], is_ru: bool):
        # å•è¯
        for t in tokens:
            norm = normalize_token(t)
            if not norm or is_stopword(norm, is_ru):
                continue
            # é¦–å­—æ¯å¤§å†™ä¼˜å…ˆ/æœ‰ä¸“åå‘³é“ï¼šç®€å•è§„åˆ™ï¼šå‡ºç°å¤§å†™æˆ–åŒ…å«ç‚¹/è¿å­—ç¬¦
            if re.match(r"[A-ZĞ-Ğ¯Ğ]", t) or "." in t or "-" in t:
                cands.add(t.strip())
        # ä¸¤è¯
        for i in range(len(tokens) - 1):
            t1, t2 = tokens[i], tokens[i+1]
            if not t1 or not t2:
                continue
            n1, n2 = normalize_token(t1), normalize_token(t2)
            if not n1 or not n2:
                continue
            if is_stopword(n1, is_ru) or is_stopword(n2, is_ru):
                continue
            phrase = f"{t1.strip()} {t2.strip()}"
            # ä»»ä¸€å«å¤§å†™/ç‚¹/è¿å­—ç¬¦å³å¯
            if (re.match(r"[A-ZĞ-Ğ¯Ğ]", t1) or re.match(r"[A-ZĞ-Ğ¯Ğ]", t2) or
                "." in phrase or "-" in phrase):
                cands.add(phrase)

    add_phrases(buf, is_ru=False)
    add_phrases(buf_ru, is_ru=True)
    # æ§åˆ¶é•¿åº¦ï¼šä¸è¶…è¿‡ä¸¤è¯
    cands_final = set()
    for p in cands:
        if 1 <= len(p.strip().split()) <= MAX_TOKENS_IN_PHRASE:
            cands_final.add(p)
    return cands_final

# ================== ä¸»å¼‚æ­¥æµç¨‹ ==================
async def main_async():
    provider = choose_provider()

    # è½½æ•°æ®
    data     = load_json(DATA_PATH, {})
    lang_map = load_json(LANG_MAP_PATH, {})
    name_map = load_json(NAME_MAP_PATH, {})

    total = len(data)
    # å®šä½åˆ—
    ru_col = pick_col_by_lang(lang_map, "Russian")
    en_col = pick_col_by_lang(lang_map, "English")
    zh_col = pick_col_by_lang(lang_map, TARGET_LANG_KEYWORD)
    if ru_col < 0 or en_col < 0 or zh_col < 0:
        print("âŒ æœªæ£€æµ‹åˆ°å®Œæ•´ä¸‰è¯­åˆ—ï¼ˆéœ€ Russian / English / Chineseï¼‰")
        return

    print(f"âœ… æ•°æ®è½½å…¥æˆåŠŸï¼šå…± {total} æ¡ã€‚ä¿„æ–‡åˆ—={ru_col}ï¼Œè‹±æ–‡åˆ—={en_col}ï¼Œä¸­æ–‡åˆ—={zh_col}")

    # ç­›é€‰å€™é€‰ï¼ˆä»…ä¸­æ–‡åˆ—éç©ºï¼Œä¸”ä¸­è‹±ä¿„å‡æœ‰å†…å®¹ï¼‰
    items: List[Tuple[str, List[str]]] = []
    for key, row in data.items():
        if not isinstance(row, list):
            continue
        ensure_row_len(row, max(ru_col, en_col, zh_col))
        ru, en, cn = (row[ru_col] or "").strip(), (row[en_col] or "").strip(), (row[zh_col] or "").strip()
        if not (ru and en and cn):
            continue
        # åªå¯¹**ä¸­æ–‡åˆ—**åšç»Ÿä¸€ï¼›ä¿ç•™åè¯è¡¨ä¸­å·²å­˜åœ¨çš„ä¼˜å…ˆ
        items.append((key, [ru, en, cn]))

    print(f"ğŸ“¦ å¾…æ£€æŸ¥å€™é€‰ï¼š{len(items)} æ¡ã€‚")
    if not items:
        print("âœ… ç»Ÿä¸€å®Œæˆï¼Œå…±ä¿®æ­£ 0 æ¡ã€‚")
        save_json(OUTPUT_PATH, data)
        save_json(NAME_MAP_PATH, name_map)
        with open(REPORT_PATH, "w", encoding="utf-8") as f:
            f.write("")
        print(f"ğŸ“˜ æŠ¥å‘Šä¿å­˜è‡³: {REPORT_PATH}")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {OUTPUT_PATH}")
        print(f"ğŸ§  æ›´æ–°ä¸“åæ˜ å°„è¡¨: {NAME_MAP_PATH}ï¼ˆæ–°å¢ 0 é¡¹ï¼‰")
        return

    # å‡†å¤‡å¼‚æ­¥ä¸Šä¸‹æ–‡
    limiter = RateLimiter(RPM, TPM)
    cache   = Cache(CACHE_PATH)
    start_ts = time.time()
    processed = 0
    modified  = 0
    new_name_pairs = {}   # æ–°å¢å…¥ name_map çš„é”®å€¼
    report_lines: List[str] = []

    # å¹¶å‘æ§åˆ¶
    sem = asyncio.Semaphore(ASYNC_CONCURRENCY)

    @asynccontextmanager
    async def get_client():
        async with build_async_client(provider) as (client, model):
            yield client, model

    async def one_job(client, model, key: str, ru: str, en: str, cn: str):
        nonlocal processed, modified
        # æå– 1~2 è¯çš„å€™é€‰çŸ­ä¸“å
        cands = extract_short_candidates(ru, en)
        if not cands:
            processed += 1
            return None

        # å¦‚æœæ‰€æœ‰å€™é€‰éƒ½å·²ç»åœ¨ä¸­æ–‡é‡Œä¸€è‡´æˆ–å·²è¢« name_map å®Œå…¨è¦†ç›–ï¼Œä¹Ÿä¸å¿…è¯·æ±‚
        need_ai = False
        for c in cands:
            # è‹¥ä¸­æ–‡é‡Œå‡ºç°å¯¹åº”è‹±æ–‡/ä¿„æ–‡æ®‹ç•™ï¼Œæˆ–åŒè¯å­˜åœ¨å¤šç§å½¢å¼ï¼Œæ‰éœ€è¦AI
            if re.search(re.escape(c), cn, flags=re.IGNORECASE):
                need_ai = True
                break
            # è‹¥ name_map æœ‰è¯¥é”®ä½†ä¸­æ–‡æœªæ›¿æ¢ï¼Œä¹Ÿéœ€è¦AIç¡®è®¤ç»Ÿä¸€ï¼ˆå…¼å®¹å¤§å°å†™ï¼‰
            if c in name_map and name_map[c] not in cn:
                need_ai = True
                break
        if not need_ai:
            processed += 1
            return None

        # ç”Ÿæˆ promptï¼ˆæ§åˆ¶ tokens ä¼°è®¡ç”¨ï¼‰
        prompt = build_ai_prompt(ru, en, cn)
        prompt_len = len(prompt)

        # ç¼“å­˜æŸ¥è¯¢
        cached = await cache.get(provider, model, ru, en, cn)
        if cached:
            mapping = clean_json_only(cached)
        else:
            # é™é€Ÿè®¸å¯
            await limiter.acquire(prompt_len)
            try:
                async with sem:
                    resp = await client.chat.completions.create(
                        model=model,
                        messages=[
                            {"role": "system", "content": "You are a multilingual localization consistency assistant. Output JSON only."},
                            {"role": "user", "content": prompt}
                        ],
                        temperature=0.2,
                        timeout=REQUEST_TIMEOUT,
                    )
                raw = (resp.choices[0].message.content or "").strip()
                mapping = clean_json_only(raw)
                await cache.set(provider, model, ru, en, cn, raw)
            except Exception as e:
                # å‘½ä¸­é™é€Ÿ -> é™é€Ÿæƒ©ç½š
                if "RateLimit" in type(e).__name__ or "429" in str(e):
                    await limiter.penalize()
                mapping = {}

        if mapping:
            # ä»…å†™ä¸­æ–‡åˆ—
            new_cn = replace_pairs_in_text(cn, mapping)
            if new_cn != cn:
                data[key][zh_col] = new_cn
                modified += 1
                # æŠ¥å‘Šå†™å…¥
                report_lines.append(f"\nåŸ: {cn}\næ–°: {new_cn}\næ˜ å°„: {json.dumps(mapping, ensure_ascii=False)}\n")
            # æ›´æ–° name_mapï¼ˆä¸è¦†ç›–æ—§å€¼ï¼‰
            changed = 0
            for k0, v0 in mapping.items():
                if k0 not in name_map:
                    name_map[k0] = v0
                    new_name_pairs[k0] = v0
                    changed += 1
            if changed:
                # ä¸ä¸­æ–­æµç¨‹ï¼Œæœ€ç»ˆç»Ÿä¸€è½ç›˜
                pass

        processed += 1
        return mapping if mapping else None

    async with get_client() as (client, model):
        tasks = []
        for key, arr in items:
            ru, en, cn = arr
            tasks.append(one_job(client, model, key, ru, en, cn))

        # åˆ†æ‰¹ gatherï¼Œé¿å…ä¸€æ¬¡æ€§åˆ›å»ºè¶…å¤§ä»»åŠ¡
        BATCH = 5000
        for chunk in chunked(tasks, BATCH):
            if stop_requested:
                break
            await asyncio.gather(*chunk)
            # å®æ—¶è½ç›˜ï¼ˆæ–­ç‚¹ä¿æŠ¤ï¼‰
            if processed % FLUSH_EVERY != 0:
                continue
            save_json(OUTPUT_PATH, data)
            save_json(NAME_MAP_PATH, name_map)
            await cache.flush()
            done_pct = processed / max(1, len(items)) * 100.0
            print(f"ğŸ’¾ è‡ªåŠ¨ä¿å­˜ | è¿›åº¦ {processed}/{len(items)} ({done_pct:.1f}%) | é¢„è®¡å‰©ä½™ {eta_str(processed, len(items), start_ts)}")

            if stop_requested:
                break

        # æœ€ç»ˆè½ç›˜
        save_json(OUTPUT_PATH, data)
        save_json(NAME_MAP_PATH, name_map)
        await cache.flush()

    # è¿›åº¦æ€»ç»“
    print(f"âœ… ç»Ÿä¸€å®Œæˆï¼Œä¿®æ­£ {modified} æ¡ã€‚æŠ¥å‘Šï¼š{REPORT_PATH}")
    with open(REPORT_PATH, "w", encoding="utf-8") as f:
        f.write("\n".join(report_lines))

# ================== å…¥å£ ==================
def main():
    print("", end="")  # è®© PowerShell å…ˆåˆ·æ–°ä¸€è¡Œ
    asyncio.run(main_async())

if __name__ == "__main__":
    main()
