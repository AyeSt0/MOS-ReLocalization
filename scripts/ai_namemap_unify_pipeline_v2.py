# scripts/ai_namemap_unify_pipeline_v2.py
# -*- coding: utf-8 -*-
import os
import re
import json
import time
import asyncio
import hashlib
import difflib
import signal
import argparse
from pathlib import Path
from typing import Dict, List, Tuple
from contextlib import asynccontextmanager
from dotenv import load_dotenv

# ========= ç¯å¢ƒ =========
load_dotenv()

# â€”â€” è·¯å¾„ï¼ˆæŒ‰ä½ çš„é¡¹ç›®ç»“æ„ï¼‰â€”â€”
DATA_PATH       = Path("output/language_dict_mcsurname_fixed.json")  # ä½ çš„æœ€æ–°ä¸­æ–‡åˆ—æ–‡ä»¶
LANG_MAP_PATH   = Path("data/language_map.json")
NAME_MAP_PATH   = Path("data/name_map.json")

CANDIDATES_PATH   = Path("output/name_ai_candidates.json")
INCONSISTENT_PATH = Path("output/name_ai_inconsistent.json")
FIX_LOG_PATH      = Path("output/name_ai_fixes.jsonl")
OUTPUT_PATH       = Path("output/language_dict_name_unified.json")
REPORT_PATH       = Path("output/name_unify_report.txt")

CACHE_PATH      = Path("cache/ai_namemap_cache.json")

# â€”â€” å¹¶å‘ / é™é€Ÿ / è½ç›˜ â€”â€”
ASYNC_CONCURRENCY = int(os.getenv("ASYNC_CONCURRENCY", "30"))    # å¹¶å‘
REQUEST_TIMEOUT   = int(os.getenv("REQUEST_TIMEOUT", "30"))
BATCH_FLUSH       = int(os.getenv("BATCH_FLUSH", "500"))
PRINT_EVERY       = int(os.getenv("PRINT_EVERY", "100"))

# â€”â€” å¼•æ“é€‰æ‹© & æ¨¡å‹ï¼ˆä¸ä½ ç°æœ‰ env ä¿æŒä¸€è‡´ï¼‰â€”â€”
DEFAULT_PROVIDER  = os.getenv("MODEL_PROVIDER", "").strip().lower()  # chatgpt / deepseek
# ChatGPT
OPENAI_API_KEY    = os.getenv("OPENAI_API_KEY", "").strip()
OPENAI_MODEL      = os.getenv("MODEL", "gpt-4o-mini").strip()
# DeepSeekï¼ˆOpenAIå…¼å®¹æ¥å£ï¼‰
DEEPSEEK_API_KEY  = os.getenv("DEEPSEEK_API_KEY", "").strip()
DEEPSEEK_BASEURL  = os.getenv("DEEPSEEK_BASE_URL", "https://api.siliconflow.cn").strip()
DEEPSEEK_MODEL    = os.getenv("DEEPSEEK_MODEL", "deepseek-ai/DeepSeek-V3.2-Exp").strip()

# â€”â€” é™é€Ÿï¼ˆDeepSeekï¼šRPM=1000 / TPM=100000ï¼Œå·²åœ¨ .env é‡Œç»™äº† 700/80000 æ›´ç¨³ï¼‰â€”â€”
RPM               = int(os.getenv("RPM", "700"))
TPM               = int(os.getenv("TPM", "80000"))

# â€”â€” ä»…å¤„ç†å«è‹±/ä¿„å­—çš„æ­£åˆ™ï¼ˆæ£€æµ‹ä¸­æ–‡é‡Œå¤¹æ‚ï¼‰â€”â€”
HAS_LATIN_OR_CYR  = re.compile(r"[A-Za-zĞ-Ğ¯Ğ°-ÑĞÑ‘]")

stop_requested = False

def handle_signal(signum, frame):
    global stop_requested
    stop_requested = True
    print("\nâš ï¸ æ•è·åˆ°ä¸­æ–­ä¿¡å·ï¼Œå°†å®‰å…¨è½ç›˜å¹¶é€€å‡ºâ€¦")
signal.signal(signal.SIGINT, handle_signal)
signal.signal(signal.SIGTERM, handle_signal)

# ========= I/O =========
def load_json(path: Path, default=None):
    if default is None: default = {}
    if not path.exists(): return default
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)

def save_json(path: Path, data):
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp = path.with_suffix(path.suffix + ".tmp")
    with tmp.open("w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    tmp.replace(path)

def append_jsonl(path: Path, record: dict):
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("a", encoding="utf-8") as f:
        f.write(json.dumps(record, ensure_ascii=False) + "\n")

def pick_col_by_lang(lang_map: Dict[str, str], label_contains: str) -> int:
    for k, v in lang_map.items():
        if v and label_contains.lower() in v.lower():
            return int(k)
    return -1

# ========= è¿›åº¦å·¥å…· =========
def sha1(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8")).hexdigest()

# ========= é€Ÿç‡é™åˆ¶å™¨ =========
class RateLimiter:
    def __init__(self, rpm: int, tpm: int):
        self.rpm = rpm
        self.tpm = tpm
        self._lock = asyncio.Lock()
        self._win_start = time.monotonic()
        self._req_used = 0
        self._tok_used = 0
        self._decay = 1.0  # åŠ¨æ€é™é€Ÿå› å­ï¼ˆå‘½ä¸­ 429 æ—¶è°ƒä½ï¼‰

    def _maybe_reset(self):
        now = time.monotonic()
        if now - self._win_start >= 60.0:
            self._win_start = now
            self._req_used = 0
            self._tok_used = 0

    def _estimate_tokens(self, ru_text: str, en_text: str, zh_text: str, std_map_json: str) -> int:
        base = len(ru_text) + len(en_text) + len(zh_text) + len(std_map_json) + 300
        return max(10, int(base / 3.5))

    async def acquire(self, ru_text: str, en_text: str, zh_text: str, std_map_json: str):
        est = self._estimate_tokens(ru_text, en_text, zh_text, std_map_json)
        while True:
            async with self._lock:
                self._maybe_reset()
                rpm_ok = (self._req_used + 1) <= int(self.rpm * self._decay)
                tpm_ok = (self._tok_used + est) <= int(self.tpm * self._decay)
                if rpm_ok and tpm_ok:
                    self._req_used += 1
                    self._tok_used += est
                    return
                wait = max(0.0, 60.0 - (time.monotonic() - self._win_start))
            await asyncio.sleep(min(1.0, wait))

    async def cool_down(self):
        async with self._lock:
            self._decay = max(0.2, self._decay * 0.85)  # å‘½ä¸­ 429 å°±æ›´ä¿å®ˆ
        await asyncio.sleep(2.0)

    async def relax(self):
        async with self._lock:
            self._decay = min(1.0, self._decay + 0.05)

# ========= å¼‚æ­¥å®¢æˆ·ç«¯ =========
@asynccontextmanager
async def build_async_client(provider: str):
    from openai import AsyncOpenAI
    if provider == "deepseek":
        if not DEEPSEEK_API_KEY:
            raise RuntimeError("ç¼ºå°‘ DEEPSEEK_API_KEY")
        client = AsyncOpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASEURL)
        model = DEEPSEEK_MODEL
        try:
            yield client, model
        finally:
            await client.close()
    else:
        if not OPENAI_API_KEY:
            raise RuntimeError("ç¼ºå°‘ OPENAI_API_KEY")
        client = AsyncOpenAI(api_key=OPENAI_API_KEY)
        model = OPENAI_MODEL
        try:
            yield client, model
        finally:
            await client.close()

def choose_provider() -> str:
    print("\nè¯·é€‰æ‹©å¼•æ“ï¼ˆ1=ChatGPTï¼Œ2=DeepSeekï¼‰ï¼š", end="")
    choice = input().strip()
    if choice == "2" or (not choice and DEFAULT_PROVIDER == "deepseek"):
        print("ğŸ§  ä½¿ç”¨ DEEPSEEK æ¨¡å‹å¼•æ“")
        return "deepseek"
    print("ğŸ§  ä½¿ç”¨ CHATGPT æ¨¡å‹å¼•æ“")
    return "chatgpt"

# ========= ç¼“å­˜ =========
class SimpleCache:
    def __init__(self, path: Path):
        self.path = path
        self._data = load_json(path, default={})
        self._lock = asyncio.Lock()

    def _key(self, provider: str, model: str, ru: str, en: str, zh: str, std_map_json: str) -> str:
        raw = f"{provider}|{model}|{ru}|{en}|{zh}|{std_map_json}"
        return sha1(raw)

    async def get(self, provider, model, ru, en, zh, std_json):
        k = self._key(provider, model, ru, en, zh, std_json)
        async with self._lock:
            return self._data.get(k, "")

    async def set(self, provider, model, ru, en, zh, std_json, val: str):
        if not val:
            return
        k = self._key(provider, model, ru, en, zh, std_json)
        async with self._lock:
            if k not in self._data:
                self._data[k] = val

    async def flush(self):
        async with self._lock:
            save_json(self.path, self._data)

# ========= æ–‡æœ¬æ¸…æ´— & å ä½ç¬¦ä¿®æ­£ =========
def clean_output(s: str) -> str:
    if not s: return ""
    s = s.strip().strip("`").strip()
    lines = [ln.strip() for ln in s.splitlines() if ln.strip()]
    return lines[0] if lines else ""

def fix_placeholders(s: str) -> str:
    # æŠŠå„ç§æ‹¬å·å½¢å¼çš„ mcname ç»Ÿä¸€æˆåŠè§’æ–¹æ‹¬å· [mcname]
    s = re.sub(r"[{\[ï¼ˆã€]\s*mcname\s*[}\]ï¼‰ã€‘]", "[mcname]", s, flags=re.IGNORECASE)
    return s

# ========= é˜¶æ®µ 1ï¼šæŠ½å–å€™é€‰ =========
def phase_extract(data: Dict[str, List[str]], lang_map: Dict[str, str], name_map: Dict[str, str]) -> List[Tuple[str, str, str, str]]:
    ru_col = pick_col_by_lang(lang_map, "Russian")
    en_col = pick_col_by_lang(lang_map, "English")
    zh_col = pick_col_by_lang(lang_map, "Chinese")
    if min(ru_col, en_col, zh_col) < 0:
        raise RuntimeError("language_map.json æœªæ£€æµ‹åˆ° Russian / English / Chinese åˆ—")

    keys = list(name_map.keys())
    candidates = []
    for key, row in data.items():
        if len(row) <= max(ru_col, en_col, zh_col):
            continue
        ru, en, zh = row[ru_col] or "", row[en_col] or "", row[zh_col] or ""
        # å‘½ä¸­ä¸“åé”®ï¼ˆä¿„æˆ–è‹±ï¼‰
        if any(k in ru or k in en for k in keys):
            candidates.append((key, ru, en, zh))

    save_json(CANDIDATES_PATH, [{"key": k, "ru": r, "en": e, "zh": z} for k, r, e, z in candidates])
    print(f"ğŸ§² å€™é€‰æŠ½å–å®Œæˆï¼š{len(candidates)} æ¡ â†’ {CANDIDATES_PATH}")
    return candidates

# ========= é˜¶æ®µ 2ï¼šæœ¬åœ°æ¨¡ç³Šæ£€æµ‹ =========
def zh_needs_fix(zh: str, std_vals: List[str]) -> bool:
    # å¦‚æœæ ‡å‡†è¯‘åå·²ç»åœ¨å¥ä¸­ï¼Œé€šå¸¸ä¸éœ€è¦ä¿®
    if any(std in zh for std in std_vals):
        return False
    # å«è‹±/ä¿„å­—ç¬¦ï¼Œå¯èƒ½æ˜¯æ²¡æ›¿å¹²å‡€
    if HAS_LATIN_OR_CYR.search(zh):
        return True
    # ä¸ä»»ä¸€æ ‡å‡†åç›¸ä¼¼åº¦è¾ƒé«˜ï¼Œè¯´æ˜å¯èƒ½æ˜¯å˜ä½“ï¼ˆä½†æœªå‘½ä¸­æ ‡å‡†æœ¬èº«ï¼‰
    for std in std_vals:
        ratio = difflib.SequenceMatcher(None, zh, std).ratio()
        if ratio > 0.35:  # å¥çº§ç²—åˆ¤é˜ˆå€¼
            return True
    return False

def phase_detect(candidates: List[Tuple[str, str, str, str]], name_map: Dict[str, str]) -> List[Tuple[str, str, str, str, Dict[str, str]]]:
    # é’ˆå¯¹æ¯æ¡å€™é€‰ï¼Œæ±‡æ€»å…¶â€œè¡Œå†…å‘½ä¸­çš„æ ‡å‡†è¡¨â€ï¼ˆå¯èƒ½å‘½ä¸­å¤šä¸ªé”® â†’ å¯èƒ½éœ€å¤šåç»Ÿä¸€ï¼‰
    result = []
    for key, ru, en, zh in candidates:
        std_pairs = {}
        for k, v in name_map.items():
            if (k in ru) or (k in en):
                std_pairs[k] = v
        if not std_pairs:
            continue
        std_values = list(set(std_pairs.values()))
        if zh_needs_fix(zh, std_values):
            result.append((key, ru, en, zh, std_pairs))

    # ä¿å­˜æ£€æµ‹æ¸…å•
    payload = [{"key": k, "ru": r, "en": e, "zh": z, "std_pairs": sp} for k, r, e, z, sp in result]
    save_json(INCONSISTENT_PATH, payload)
    print(f"ğŸ” æœ¬åœ°æ£€æµ‹å®Œæˆï¼šç–‘ä¼¼ä¸ç»Ÿä¸€ {len(result)} æ¡ â†’ {INCONSISTENT_PATH}")
    return result

# ========= AI ä¿®æ­£ =========
def build_fix_prompt(ru: str, en: str, zh: str, std_pairs: Dict[str, str]) -> str:
    # åªæ›¿æ¢ä¸“åï¼›ä¸æ”¹å…¶ä»–å†…å®¹ï¼›è¾“å‡ºå®Œæ•´ä¿®æ­£ä¸­æ–‡
    std_json = json.dumps(std_pairs, ensure_ascii=False)
    return (
        "ä½ æ˜¯ä¸­æ–‡æœ¬åœ°åŒ–ä¸€è‡´æ€§ä¿®æ­£åŠ©æ‰‹ã€‚è¯·ä»…åœ¨ä¸‹åˆ—ä¸­æ–‡å¥å­ä¸­ï¼Œå°†ä¸“æœ‰åè¯ç»Ÿä¸€ä¸ºç»™å®šçš„æ ‡å‡†è¯‘åã€‚\n"
        "è¦æ±‚ï¼š\n"
        "1) åªæ›¿æ¢å¯¹åº”ä¸“åçš„å„ç§å˜ä½“ä¸ºæ ‡å‡†è¯‘åï¼›ä¸æ”¹å˜å…¶ä»–æ–‡å­—ã€è¯­åºä¸æ ‡ç‚¹ã€‚\n"
        "2) ä¿ç•™å ä½ç¬¦ä¸æ ‡ç­¾ï¼ˆå¦‚ [mcname]ã€{å˜é‡}ã€<tag> ç­‰ï¼‰åŸæ ·ã€‚\n"
        "3) ä¸è¦è¾“å‡ºè§£é‡Šæˆ–æ³¨é‡Šï¼Œåªè¾“å‡ºæœ€åçš„å®Œæ•´ä¸­æ–‡å¥å­ã€‚\n\n"
        f"ä¸“å-æ ‡å‡†è¯‘åè¡¨ï¼š{std_json}\n\n"
        f"ä¿„æ–‡ï¼š{ru}\n"
        f"è‹±æ–‡ï¼š{en}\n"
        f"ä¸­æ–‡åŸå¥ï¼š\n{zh}\n"
        "è¾“å‡ºï¼š"
    )

async def ai_fix_once(client, model: str, limiter: RateLimiter, provider: str,
                      ru: str, en: str, zh: str, std_pairs: Dict[str, str]) -> str:
    std_json = json.dumps(std_pairs, ensure_ascii=False)
    await limiter.acquire(ru, en, zh, std_json)

    sys_msg  = "You are a precise Chinese localization consistency assistant. Output only the corrected Chinese line."
    user_msg = build_fix_prompt(ru, en, zh, std_pairs)

    for attempt in range(1, 6):
        if stop_requested:
            return ""
        try:
            resp = await client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": sys_msg},
                    {"role": "user", "content": user_msg},
                ],
                temperature=0.1,
                timeout=REQUEST_TIMEOUT,
            )
            out = clean_output(resp.choices[0].message.content or "")
            out = fix_placeholders(out)
            await limiter.relax()
            return out
        except Exception as e:
            ename = type(e).__name__
            estr = str(e)
            if "429" in estr or "RateLimit" in ename:
                print(f"âš ï¸ RateLimitï¼šå†·å´ä¸­ï¼ˆattempt {attempt})")
                await limiter.cool_down()
                await asyncio.sleep(min(60.0, 2 ** attempt))
                continue
            wait = min(30.0, 1.8 ** attempt)
            print(f"â³ é‡è¯• {attempt}/5ï¼š{ename}ï¼Œ{wait:.1f}s åå†è¯•â€¦")
            await asyncio.sleep(wait)
    return ""

# ========= é˜¶æ®µ 3ï¼šAI æ‰¹é‡ä¿®æ­£å¹¶è½ç›˜ =========
async def phase_fix(data: Dict[str, List[str]], lang_map: Dict[str, str],
                    inconsistent: List[Tuple[str, str, str, str, Dict[str, str]]],
                    provider: str):

    zh_col = pick_col_by_lang(lang_map, "Chinese")
    if zh_col < 0:
        raise RuntimeError("language_map.json æœªæ£€æµ‹åˆ° Chinese åˆ—")

    limiter = RateLimiter(RPM, TPM)
    cache   = SimpleCache(CACHE_PATH)
    fixed = 0
    total = len(inconsistent)
    last_flush = 0

    async with build_async_client(provider) as (client, model):
        sem = asyncio.Semaphore(ASYNC_CONCURRENCY)

        async def worker(i: int, key: str, ru: str, en: str, zh: str, std_pairs: Dict[str, str]):
            nonlocal fixed, last_flush
            std_json = json.dumps(std_pairs, ensure_ascii=False)
            cached = await cache.get(provider, model, ru, en, zh, std_json)
            if cached:
                new_zh = cached
            else:
                async with sem:
                    new_zh = await ai_fix_once(client, model, limiter, provider, ru, en, zh, std_pairs)
                await cache.set(provider, model, ru, en, zh, std_json, new_zh)

            # å†™å›ï¼ˆä»…ä¸­æ–‡åˆ—ï¼‰
            if new_zh and new_zh != zh:
                data[key][zh_col] = new_zh
                fixed += 1
                append_jsonl(FIX_LOG_PATH, {
                    "idx": i, "key": key, "ru": ru, "en": en,
                    "zh_old": zh, "zh_new": new_zh, "std_pairs": std_pairs
                })

            # è¿›åº¦
            if i % PRINT_EVERY == 0:
                preview_src = (zh[:28] + "â€¦") if len(zh) > 28 else zh
                preview_new = (new_zh[:28] + "â€¦") if new_zh and len(new_zh) > 28 else (new_zh or "")
                print(f"â³ [{i}/{total}] {preview_src} â†’ {preview_new}")

            # æ‰¹é‡è½ç›˜
            if i - last_flush >= BATCH_FLUSH:
                save_json(OUTPUT_PATH, data)
                await cache.flush()
                last_flush = i
                print("ğŸ’¾ è‡ªåŠ¨ä¿å­˜è¿›åº¦â€¦")

        tasks = [worker(i, k, ru, en, zh, sp) for i, (k, ru, en, zh, sp) in enumerate(inconsistent, 1)]
        try:
            for chunk_start in range(0, len(tasks), 10000):
                chunk = tasks[chunk_start:chunk_start+10000]
                await asyncio.gather(*chunk)
                if stop_requested:
                    break
        finally:
            save_json(OUTPUT_PATH, data)
            await cache.flush()

    print(f"âœ… AI ä¿®æ­£å®Œæˆï¼šå…±æ›¿æ¢ {fixed}/{total} æ¡ â†’ {OUTPUT_PATH}")
    return fixed

# ========= ä¸»å…¥å£ï¼ˆå¤šé˜¶æ®µï¼‰ =========
def main():
    parser = argparse.ArgumentParser(description="ä¸“åç»Ÿä¸€ç®¡çº¿ï¼ˆåŸºäº name_mapï¼Œä¸‰é˜¶æ®µï¼šextract / detect / fix / allï¼‰")
    parser.add_argument("--phase", type=str, default="all",
                        choices=["extract", "detect", "fix", "all"],
                        help="é€‰æ‹©æ‰§è¡Œé˜¶æ®µ")
    args = parser.parse_args()

    provider = choose_provider()

    data     = load_json(DATA_PATH, default={})
    lang_map = load_json(LANG_MAP_PATH, default={})
    name_map = load_json(NAME_MAP_PATH, default={})

    total = len(data)
    ru_col = pick_col_by_lang(lang_map, "Russian")
    en_col = pick_col_by_lang(lang_map, "English")
    zh_col = pick_col_by_lang(lang_map, "Chinese")

    if min(ru_col, en_col, zh_col) < 0:
        print("âŒ æœªæ£€æµ‹åˆ°å®Œæ•´çš„ä¸‰è¯­åˆ—ï¼ˆRussian/English/Chineseï¼‰ï¼Œè¯·æ£€æŸ¥ language_map.json")
        return

    print(f"âœ… æ•°æ®è½½å…¥æˆåŠŸï¼šå…± {total} æ¡ã€‚ä¿„æ–‡åˆ—={ru_col}ï¼Œè‹±æ–‡åˆ—={en_col}ï¼Œä¸­æ–‡åˆ—={zh_col}")
    print(f"ğŸ“˜ name_map ä¸­æœ‰ {len(name_map)} æ¡ä¸“åæ˜ å°„ã€‚")
    print(f"ğŸ” æ£€æµ‹ä¸“åï¼šå…± {len(set(name_map.keys()))} ä¸ª")

    if args.phase in ("extract", "all"):
        candidates = phase_extract(data, lang_map, name_map)
    else:
        # è‹¥ä¸æ˜¯ extractï¼Œä»æ–‡ä»¶å¤ç”¨
        if CANDIDATES_PATH.exists():
            cjson = load_json(CANDIDATES_PATH, default=[])
            candidates = [(x["key"], x["ru"], x["en"], x["zh"]) for x in cjson]
        else:
            candidates = phase_extract(data, lang_map, name_map)

    if args.phase in ("detect", "all"):
        inconsistent = phase_detect(candidates, name_map)
    else:
        # è‹¥ä¸æ˜¯ detectï¼Œä»æ–‡ä»¶å¤ç”¨
        if INCONSISTENT_PATH.exists():
            incjson = load_json(INCONSISTENT_PATH, default=[])
            inconsistent = [(x["key"], x["ru"], x["en"], x["zh"], x["std_pairs"]) for x in incjson]
        else:
            inconsistent = phase_detect(candidates, name_map)

    print(f"ğŸ“¦ å¾…ä¿®æ­£å¥å­æ•°ï¼š{len(inconsistent)}ï¼ˆä»…æ£€æµ‹è‹±æ–‡/ä¿„æ–‡å«ä¸“åè¡Œï¼‰")

    if args.phase in ("fix", "all") and inconsistent:
        asyncio.run(phase_fix(data, lang_map, inconsistent, provider))
        # æœ€ç»ˆæŠ¥å‘Š
        report = [
            f"æ€»è®°å½•æ•°ï¼š{total}",
            f"å€™é€‰ï¼ˆå«ä¸“åï¼‰è¡Œï¼š{len(candidates)}",
            f"ç–‘ä¼¼ä¸ç»Ÿä¸€è¡Œï¼š{len(inconsistent)}",
            f"è¾“å‡ºï¼š{OUTPUT_PATH}",
            f"ä¿®å¤æ—¥å¿—ï¼š{FIX_LOG_PATH}"
        ]
        REPORT_PATH.parent.mkdir(parents=True, exist_ok=True)
        REPORT_PATH.write_text("\n".join(report), encoding="utf-8")
        print(f"ğŸ“˜ æŠ¥å‘Šä¿å­˜è‡³: {REPORT_PATH}")
    else:
        # é fix é˜¶æ®µä¹Ÿè½åº“ä¸€ä»½æœªæ”¹åŠ¨çš„æ•°æ®ï¼Œä¾¿äºæ¯”å¯¹
        save_json(OUTPUT_PATH, data)
        print(f"ğŸ“ å·²ä¿å­˜å½“å‰æ•°æ®å¿«ç…§ï¼ˆæœªæ”¹åŠ¨ï¼‰ï¼š{OUTPUT_PATH}")

if __name__ == "__main__":
    main()
