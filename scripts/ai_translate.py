# scripts/ai_translate.py
import os
import sys
import json
import time
import signal
import re
import asyncio
import hashlib
from pathlib import Path
from dotenv import load_dotenv
from typing import Dict, Tuple, List
from contextlib import asynccontextmanager

# ================== ç¯å¢ƒä¸å¸¸é‡ ==================
load_dotenv()

# è·¯å¾„
DATA_PATH        = Path("data/language_dict.json")
LANG_MAP_PATH    = Path("data/language_map.json")
NAME_MAP_PATH    = Path("data/name_map.json")
OUTPUT_PATH      = Path("output/language_dict_translated.json")
CACHE_PATH       = Path("cache/cache.json")

# å¹¶å‘ / é™é€Ÿ / è½ç›˜
ASYNC_CONCURRENCY = int(os.getenv("ASYNC_CONCURRENCY", "100"))   # å¹¶å‘åº¦ï¼ˆDeepSeek å¯æ‹‰é«˜ï¼‰
REQUEST_TIMEOUT   = int(os.getenv("REQUEST_TIMEOUT", "30"))
BATCH_FLUSH       = int(os.getenv("BATCH_FLUSH", "200"))         # å¤šå°‘æ¡è½ç›˜ä¸€æ¬¡
PRINT_EVERY       = int(os.getenv("PRINT_EVERY", "50"))          # å¤šå°‘æ¡æ‰“å°ä¸€æ¬¡ç®€è¦è¿›åº¦

# å¼•æ“é€‰æ‹©
DEFAULT_PROVIDER  = os.getenv("MODEL_PROVIDER", "").strip().lower()  # chatgpt / deepseek
# ChatGPT
OPENAI_API_KEY    = os.getenv("OPENAI_API_KEY", "").strip()
OPENAI_MODEL      = os.getenv("MODEL", "gpt-4o-mini").strip()
# DeepSeekï¼ˆOpenAIå…¼å®¹ï¼‰
DEEPSEEK_API_KEY  = os.getenv("DEEPSEEK_API_KEY", "").strip()
DEEPSEEK_BASEURL  = os.getenv("DEEPSEEK_BASE_URL", "https://api.siliconflow.cn").strip()
DEEPSEEK_MODEL    = os.getenv("DEEPSEEK_MODEL", "deepseek-ai/DeepSeek-R1").strip()

# é™é€Ÿå‚æ•°ï¼ˆå¯åœ¨ .env è¦†ç›–ï¼‰
# DeepSeek å®˜æ–¹ï¼šRPM=1000ï¼›TPM=100000
RPM               = int(os.getenv("RPM", "1000"))
TPM               = int(os.getenv("TPM", "100000"))

# å§‹ç»ˆå¯ç”¨ä¸“åé¢„æ›¿æ¢ï¼ˆä½†ç¿»è¯‘é€»è¾‘å·²ä¼˜åŒ–ï¼šä»…å·²ç™»è®°çš„ä¸“åå¼ºåˆ¶ï¼›å…¶ä½™æ ¹æ®ä¸Šä¸‹æ–‡ï¼‰
ALWAYS_PREMAP_NAMES = True

# Honorific/å¤§å†™åœç”¨è¯ï¼ˆé¿å…æŠŠæ•¬ç§°å½“æˆä¸“åï¼‰
HONORIFICS = {
    "Mr", "Mrs", "Ms", "Miss", "Dr", "Prof", "Professor", "Coach",
    "Sir", "Madam", "Lady", "Lord", "Captain", "Principal", "Dean",
    "I", "The", "A", "An", "OK", "TV", "USA", "EU", "ID"
}

stop_requested = False

# ================== å·¥å…·å‡½æ•° ==================
def load_json(path: Path, default=None):
    if default is None:
        default = {}
    if not path.exists():
        return default
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)

def save_json(path: Path, data):
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp = path.with_suffix(path.suffix + ".tmp")
    with tmp.open("w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    tmp.replace(path)

def ensure_row_len(row: list, length: int):
    if len(row) <= length:
        row.extend([""] * (length - len(row) + 1))

def non_empty_count_of_col(data: dict, col_idx: int) -> int:
    cnt = 0
    for arr in data.values():
        if len(arr) > col_idx and str(arr[col_idx]).strip():
            cnt += 1
    return cnt

def pick_col_by_lang(lang_map: Dict[str, str], label_contains: str) -> int:
    for k, v in lang_map.items():
        if v and label_contains.lower() in v.lower():
            return int(k)
    return -1

def sha1(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8")).hexdigest()

# ================== ç»ˆæ­¢ä¿¡å· ==================
def handle_signal(signum, frame):
    global stop_requested
    stop_requested = True
    print("\nâš ï¸ æ”¶åˆ°ä¸­æ–­ï¼Œå‡†å¤‡è½ç›˜å¹¶å®‰å…¨é€€å‡ºâ€¦")

signal.signal(signal.SIGINT, handle_signal)
signal.signal(signal.SIGTERM, handle_signal)

# ================== Prompt ==================
# æˆäººå‘æœ¬åœ°åŒ–ï¼šä¿„æ–‡æƒ…ç»ª/è¯­æ°” + è‹±æ–‡è¯­ä¹‰ â†’ æµç•…ã€æ’©äººã€å¸¦èº«ä½“æ„Ÿçš„ä¸­æ–‡
def build_prompt(ru_text: str, en_text: str, target_lang_label: str) -> str:
    return f"""You are an experienced localization writer for adult visual novels.
Your job: translate & localize the following Russian and English lines into seductive,
natural, and immersive Chinese ({target_lang_label}) for the game "MILFs of Sunville".

Use English for literal meaning. Use Russian for mood and sensual nuance.
Localize for Chinese players: smooth rhythm, emotional tension, and erotic subtext â€” not literal.

Rules:
- Keep sexual details explicit but tasteful; no censorship or summary.
- Keep placeholders exactly ({{{{mcname}}}}, [var], <tag>).
- Translate person/place names only if they already have a fixed Chinese form in the given text.
- Output ONLY the final translation â€” no explanations, no quotes.

Russian: {ru_text}
English: {en_text}""".strip()

# ================== æ–‡æœ¬æ¸…æ´— ==================
def clean_model_output(s: str) -> str:
    if not s:
        return ""
    s = s.strip().strip("`").strip()
    # åªå–ç¬¬ä¸€è¡Œéç©ºå†…å®¹ï¼ˆé¿å…æ¨¡å‹å¤šä½™è§£é‡Šï¼‰
    lines = [ln.strip() for ln in s.splitlines() if ln.strip()]
    return lines[0] if lines else ""

# å˜é‡/æ ‡ç­¾ä¿æŠ¤ï¼ˆåˆ¤å®šæ˜¯å¦åŒ…å«æ•æ„Ÿå ä½ï¼‰
def has_protected_tokens(s: str) -> bool:
    return any(tok in s for tok in ("{", "}", "[", "]", "<", ">"))

# ================== åç§°é¢„æ›¿æ¢ï¼ˆä»…å·²ç™»è®°ä¸“åï¼‰ ==================
def apply_name_map_pre(src_text: str, name_map: Dict[str, str]) -> str:
    if not ALWAYS_PREMAP_NAMES or not name_map:
        return src_text
    # é•¿åŒ¹é…ä¼˜å…ˆï¼Œé¿å…çŸ­è¯è¯¯æ›¿
    for k, v in sorted(name_map.items(), key=lambda kv: -len(kv[0])):
        if not k or not v:
            continue
        # ä»…æ›¿æ¢â€œå®Œæ•´è¯/çŸ­è¯­â€å‡ºç°çš„æƒ…å†µï¼Œä¸åŠ¨å˜é‡åŠæ ‡ç­¾å†…éƒ¨
        if k in src_text and not has_protected_tokens(k):
            src_text = src_text.replace(k, v)
    return src_text

# ================== é€Ÿç‡é™åˆ¶å™¨ï¼ˆRPM & TPMï¼‰ ==================
class RateLimiter:
    """
    ç®€å•ä»¤ç‰Œæ¡¶ï¼šæ¯åˆ†é’Ÿè¯·æ±‚ä¸Šé™ RPMï¼›æ¯åˆ†é’Ÿä»¤ç‰Œä¸Šé™ TPMã€‚
    è¯·æ±‚åˆ°æ¥æ—¶ï¼Œå¦‚æœè¶…é™åˆ™ç­‰å¾…è‡³ä¸‹ä¸€çª—å£å¯ç”¨ã€‚
    """
    def __init__(self, rpm: int, tpm: int):
        self.rpm = rpm
        self.tpm = tpm
        self._lock = asyncio.Lock()
        self._window_start = time.monotonic()
        self._req_used = 0
        self._tok_used = 0

    def _maybe_reset(self):
        now = time.monotonic()
        if now - self._window_start >= 60.0:
            self._window_start = now
            self._req_used = 0
            self._tok_used = 0

    def _estimate_tokens(self, ru_text: str, en_text: str) -> int:
        # è¿‘ä¼¼ä¼°ç®— tokensï¼šä¸­è‹±ä¿„æ··åˆç²—ä¼°ï¼ˆä¿å®ˆä¸€ç‚¹ï¼‰
        # å‡è®¾ 1 token â‰ˆ 4 charsï¼ˆè‹±æ–‡ï¼‰ï¼Œä¿„æ–‡æ›´å¯†é›†ï¼Œå– 3.5ã€‚ç»Ÿä¸€å– 3.5 æ›´ç¨³å¦¥
        ln = len(ru_text) + len(en_text) + 180  # +prompt å¼€é”€ç²—ä¼°
        return max(8, int(ln / 3.5))

    async def acquire(self, ru_text: str, en_text: str):
        est = self._estimate_tokens(ru_text, en_text)
        while True:
            async with self._lock:
                self._maybe_reset()
                can_req = (self._req_used + 1) <= self.rpm
                can_tok = (self._tok_used + est) <= self.tpm
                if can_req and can_tok:
                    self._req_used += 1
                    self._tok_used += est
                    return  # è®¸å¯é€šè¿‡
                # å¦åˆ™è®¡ç®—ç­‰å¾…æ—¶é—´ï¼šç›´åˆ° 60s çª—å£é‡ç½®
                wait = max(0.0, 60.0 - (time.monotonic() - self._window_start))
            await asyncio.sleep(min(1.0, wait))

# ================== å®¢æˆ·ç«¯é€‚é…ï¼ˆå¼‚æ­¥ï¼‰ ==================
@asynccontextmanager
async def build_async_client(provider: str):
    from openai import AsyncOpenAI
    if provider == "deepseek":
        if not DEEPSEEK_API_KEY:
            raise RuntimeError("ç¼ºå°‘ DEEPSEEK_API_KEYï¼Œè¯·åœ¨ .env ä¸­é…ç½®ã€‚")
        client = AsyncOpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASEURL)
        model = DEEPSEEK_MODEL
        yield client, model
        await client.close()
    else:
        if not OPENAI_API_KEY:
            raise RuntimeError("ç¼ºå°‘ OPENAI_API_KEYï¼Œè¯·åœ¨ .env ä¸­é…ç½®ã€‚")
        client = AsyncOpenAI(api_key=OPENAI_API_KEY)
        model = OPENAI_MODEL
        yield client, model
        await client.close()

def choose_provider() -> str:
    print("\nè¯·é€‰æ‹©ç¿»è¯‘å¼•æ“ï¼š")
    print("  1) ChatGPT (OpenAI)")
    print("  2) DeepSeek (OpenAIå…¼å®¹, RPM=1000, TPM=100000)")
    default_hint = f"(é»˜è®¤: {DEFAULT_PROVIDER or 'ChatGPT'})"
    choice = input(f"ğŸ‘‰ è¾“å…¥ 1 æˆ– 2 {default_hint}ï¼š").strip()
    if choice == "2" or (not choice and DEFAULT_PROVIDER == "deepseek"):
        provider = "deepseek"
    else:
        provider = "chatgpt"
    print(f"ğŸ§  ä½¿ç”¨ {('DeepSeek' if provider=='deepseek' else 'ChatGPT')} æ¨¡å‹å¼•æ“")
    return provider

# ================== ç¼“å­˜ ==================
class SimpleCache:
    def __init__(self, path: Path):
        self.path = path
        self._data = load_json(path, default={})
        self._lock = asyncio.Lock()

    def _key(self, provider: str, model: str, tgt_label: str, ru_text: str, en_text: str) -> str:
        raw = f"{provider}|{model}|{tgt_label}|{ru_text}|{en_text}"
        return sha1(raw)

    async def get(self, provider: str, model: str, tgt_label: str, ru_text: str, en_text: str) -> str:
        k = self._key(provider, model, tgt_label, ru_text, en_text)
        async with self._lock:
            return self._data.get(k, "")

    async def set(self, provider: str, model: str, tgt_label: str, ru_text: str, en_text: str, value: str):
        if not value:
            return
        k = self._key(provider, model, tgt_label, ru_text, en_text)
        async with self._lock:
            if k not in self._data:
                self._data[k] = value

    async def flush(self):
        async with self._lock:
            save_json(self.path, self._data)

# ================== ç¿»è¯‘æ ¸å¿ƒï¼ˆå¼‚æ­¥ï¼‰ ==================
async def translate_once(client, model: str, limiter: RateLimiter, provider: str,
                         ru_text: str, en_text: str, target_lang_label: str) -> str:
    # é™é€Ÿï¼šå…ˆå–è®¸å¯
    await limiter.acquire(ru_text, en_text)

    # ç”Ÿæˆæ¶ˆæ¯
    sys_msg  = "You are a professional adult-visual-novel localization translator. Output translation only."
    user_msg = build_prompt(ru_text, en_text, target_lang_label)

    # è°ƒç”¨
    for attempt in range(1, 6):
        if stop_requested:
            return ""
        try:
            resp = await client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": sys_msg},
                    {"role": "user", "content": user_msg},
                ],
                temperature=0.6,
                timeout=REQUEST_TIMEOUT,
            )
            out = resp.choices[0].message.content or ""
            return clean_model_output(out)
        # åœ¨ translate_once çš„ except Exception å—é‡ŒåŠ ï¼š
        except Exception as e:
            if "RateLimitError" in type(e).__name__ or "429" in str(e):
                cooldown = min(60.0, 2 ** attempt)
                print(f"âš ï¸ å…¨å±€å†·å´ {cooldown:.1f}sï¼šRateLimitErrorï¼ˆå·²è§¦å‘é˜²æŠ–æœºåˆ¶ï¼‰")
                await asyncio.sleep(cooldown)
            else:
                wait = min(30.0, 1.8 ** attempt)
                print(f"â³ é‡è¯• {attempt}/5ï¼š{type(e).__name__}ï¼Œ{wait:.1f}s åå†è¯•...")
                await asyncio.sleep(wait)
    return ""

# ================== ä¸»æµç¨‹ ==================
async def main_async():
    # é€‰æ‹©å¼•æ“
    provider = choose_provider()

    # è¯»æ•°æ®
    data     = load_json(DATA_PATH, default={})
    lang_map = load_json(LANG_MAP_PATH, default={})
    name_map = load_json(NAME_MAP_PATH, default={})
    cache    = SimpleCache(CACHE_PATH)

    total = len(data)
    print(f"âœ… åŠ è½½å®Œæˆï¼Œå…± {total} æ¡è®°å½•ã€‚\n")

    # å®šä½åˆ—ï¼šä¿„æ–‡ã€è‹±æ–‡
    ru_col = pick_col_by_lang(lang_map, "Russian")
    en_col = pick_col_by_lang(lang_map, "English")
    if ru_col < 0 or en_col < 0:
        print("âŒ æ— æ³•è¯†åˆ«æºè¯­è¨€æˆ–ç›®æ ‡è¯­è¨€åˆ—ï¼ˆéœ€è¦ Russian ä¸ Englishï¼‰ï¼Œè¯·æ£€æŸ¥ language_map.json")
        return

    # å±•ç¤ºå¯ç¿»è¯‘åˆ—ï¼ˆæ’é™¤ METAï¼‰
    print("å¯ç¿»è¯‘åˆ—å¦‚ä¸‹ï¼š")
    for k, v in sorted(lang_map.items(), key=lambda kv: int(kv[0])):
        if v == "META":
            continue
        col_idx = int(k)
        pct = (non_empty_count_of_col(data, col_idx) / total * 100.0) if total else 0.0
        print(f"  - åˆ— {k}: {v} ({pct:.1f}%)")

    # Unknown åˆ—æç¤º
    unknowns = [(int(k), v) for k, v in lang_map.items() if v == "Unknown"]
    if unknowns:
        print("\nğŸŸ¡ æ£€æµ‹åˆ° Unknown åˆ—ï¼Œå¯é€‰æ‹©åˆ›å»ºæ–°è¯­è¨€ç¿»è¯‘ï¼š")
        for k, v in sorted(unknowns, key=lambda x: x[0]):
            pct = (non_empty_count_of_col(data, k) / total * 100.0) if total else 0.0
            print(f"  - åˆ— {k}: {v} ({pct:.1f}%)")

    # é€‰æ‹©ç›®æ ‡åˆ—æˆ–æ–°å¢
    raw = input("\nğŸ‘‰ è¯·è¾“å…¥è¦è¿›è¡Œæœ¬åœ°åŒ–ç¿»è¯‘çš„ç›®æ ‡åˆ—å·ï¼ˆç•™ç©ºä»¥æ–°å¢è¯­è¨€åˆ—ï¼‰ï¼š").strip()
    if raw == "":
        want_col  = input("ğŸ†• è¯·è¾“å…¥è¦æ–°å¢çš„åˆ—å·ï¼ˆç•™ç©ºåˆ™è¿½åŠ åˆ°æœ«å°¾ï¼‰ï¼š").strip()
        lang_name = input("ğŸ†• è¯·è¾“å…¥æ–°å¢åˆ—çš„è¯­è¨€åï¼ˆä¾‹å¦‚ Chinese (Simplified Chinese)ï¼‰ï¼š").strip() or "Unknown"
        max_col = max(map(int, lang_map.keys())) if lang_map else -1
        if want_col:
            tgt_col = int(want_col)
            if tgt_col > max_col + 1:
                for c in range(max_col + 1, tgt_col):
                    lang_map[str(c)] = "Unknown"
            lang_map[str(tgt_col)] = lang_name
        else:
            tgt_col = max_col + 1
            lang_map[str(tgt_col)] = lang_name
        save_json(LANG_MAP_PATH, lang_map)
        print(f"ğŸ†• æ–°å¢åˆ— {tgt_col}: {lang_map[str(tgt_col)]}")
    else:
        tgt_col = int(raw)

    target_lang_label = lang_map.get(str(tgt_col), "Unknown")
    print(f"\nğŸŒ å°†ä»åˆ— {ru_col}ï¼ˆRussianï¼‰ + åˆ— {en_col}ï¼ˆEnglishï¼‰ ç¿»åˆ° åˆ— {tgt_col}ï¼ˆ{target_lang_label}ï¼‰")

    # æ¨¡å¼
    mode = input("\né€‰æ‹©æ¨¡å¼ï¼š1=ç»§ç»­ç¿»è¯‘ï¼ˆè¡¥ç©ºï¼‰ / 2=å¼ºåˆ¶ç¿»è¯‘ï¼ˆæ¸…ç©ºé‡æ¥ï¼‰ï¼š").strip()
    if mode == "2":
        confirm = input("âš ï¸ ç¡®è®¤è¦æ¸…ç©ºè¯¥åˆ—çš„æ‰€æœ‰ç¿»è¯‘å—ï¼Ÿ(y/n)ï¼š").strip().lower()
        if confirm == "y":
            for row in data.values():
                ensure_row_len(row, tgt_col)
                row[tgt_col] = ""
            save_json(DATA_PATH, data)
            print("ğŸ§¹ å·²æ¸…ç©ºç›®æ ‡åˆ—ã€‚")
        else:
            print("å·²å–æ¶ˆæ¸…ç©ºæ“ä½œï¼Œè½¬ä¸ºç»§ç»­ç¿»è¯‘ï¼ˆè¡¥ç©ºï¼‰æ¨¡å¼ã€‚")
            mode = "1"

    # æ„å»ºä»»åŠ¡ï¼šåªç¿»è¯‘â€œæºæœ‰å†…å®¹ ä¸” ç›®æ ‡ä¸ºç©ºâ€çš„è¡Œ
    todo: List[Tuple[int, str, str, str]] = []
    index = 0
    for key, row in data.items():
        index += 1
        ensure_row_len(row, max(ru_col, en_col, tgt_col))
        ru_text = (row[ru_col] or "").strip()
        en_text = (row[en_col] or "").strip()
        tgt_text = (row[tgt_col] or "").strip()
        # åªæœ‰æºæ–‡æœ¬å­˜åœ¨æ‰ç¿»ï¼›ç»§ç»­æ¨¡å¼ï¼šä»…è¡¥ç©ºï¼›å¼ºåˆ¶æ—¶ï¼šä¸Šé¢å·²æ¸…ç©º
        if ru_text or en_text:
            if mode == "1" and tgt_text:
                continue
            todo.append((index, key, ru_text, en_text))

    print(f"\nğŸ“¦ å¾…ç¿»è¯‘: {len(todo)} æ¡ã€‚")

    if not todo:
        save_json(OUTPUT_PATH, data)
        print(f"ğŸ‰ ç¿»è¯‘å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ {OUTPUT_PATH}")
        return

    # æ·±/æµ…å¼•æ“é™é€Ÿï¼ˆè‹¥ä½¿ç”¨ ChatGPTï¼Œå¯åœ¨ .env å•ç‹¬é…ç½® RPM/TPMï¼›å¦åˆ™ç”¨é»˜è®¤ï¼‰
    limiter = RateLimiter(RPM, TPM)

    processed = 0
    last_flush = 0

    # å¼‚æ­¥å®¢æˆ·ç«¯
    async with build_async_client(provider) as (client, model):
        sem = asyncio.Semaphore(ASYNC_CONCURRENCY)

        async def one_job(idx: int, key: str, ru_text: str, en_text: str):
            nonlocal processed, last_flush

            # åç§°é¢„æ›¿æ¢ï¼ˆä»…é’ˆå¯¹è‹±æ–‡åˆ—ï¼Œä¿„æ–‡åˆ—ä¸æ›¿ï¼Œé¿å…ç ´åè¯­å¢ƒï¼‰
            en_pre = apply_name_map_pre(en_text, name_map) if en_text else en_text
            ru_pre = ru_text  # ä¿„æ–‡ä¸é¢„æ›¿

            # ç¼“å­˜æŸ¥è¯¢
            cached = await cache.get(provider, model, target_lang_label, ru_pre, en_pre)
            if cached:
                out = cached
            else:
                # é™é€Ÿ + è°ƒç”¨
                async with sem:
                    out = await translate_once(client, model, limiter, provider, ru_pre, en_pre, target_lang_label)
                out = clean_model_output(out)
                await cache.set(provider, model, target_lang_label, ru_pre, en_pre, out)

            # å†™å›
            row = data[key]
            ensure_row_len(row, tgt_col)
            row[tgt_col] = out

            processed += 1
            if processed % PRINT_EVERY == 0:
                short_src = (en_text or ru_text)[:60].replace("\n", " ")
                short_out = (out or "")[:60].replace("\n", " ")
                print(f"ğŸ”„ è¿›åº¦ {processed}/{len(todo)} | æº: {short_src} | è¯‘: {short_out}")

            # æ‰¹é‡è½ç›˜
            if processed - last_flush >= BATCH_FLUSH:
                save_json(OUTPUT_PATH, data)
                await cache.flush()
                last_flush = processed
                print(f"ğŸ’¾ è‡ªåŠ¨ä¿å­˜è¿›åº¦ -> {OUTPUT_PATH}")

        tasks = [one_job(idx, key, ru, en) for (idx, key, ru, en) in todo]

        try:
            for chunk_start in range(0, len(tasks), 10000):
                # åˆ†å—å¹¶å‘ï¼Œé¿å…è¿‡å¤šä»»åŠ¡ä¸€æ¬¡æ€§æ³¨å…¥äº‹ä»¶å¾ªç¯
                chunk = tasks[chunk_start:chunk_start+10000]
                await asyncio.gather(*chunk)
                if stop_requested:
                    break
        finally:
            # æœ€ç»ˆè½ç›˜
            save_json(OUTPUT_PATH, data)
            await cache.flush()

    print(f"\nğŸ‰ ç¿»è¯‘å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ {OUTPUT_PATH}")

def choose_provider_cli_default() -> str:
    # ä¾›å¤–éƒ¨è„šæœ¬è°ƒç”¨ï¼ˆä¿æŒè¡Œä¸ºä¸€è‡´ï¼‰
    return choose_provider()

def main():
    asyncio.run(main_async())

if __name__ == "__main__":
    main()
